{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uniform-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from boundariesdetectioncnn.data import dataloaders\n",
    "from boundariesdetectioncnn.models import model_CNN_MLS\n",
    "from boundariesdetectioncnn import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "organized-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, \n",
    "               trainloader,\n",
    "               valloader,\n",
    "               device,\n",
    "               save_epoch=5,\n",
    "               epochs=configs.ParamsConfig.NUM_EPOCHS, \n",
    "               lr=configs.ParamsConfig.LEARNING_RATE, \n",
    "               iterations=configs.ParamsConfig.ITERATIONS,\n",
    "               lamda=configs.InputsConfig.LAMBDA,\n",
    "               padding_factor=configs.InputsConfig.PADDING_FACTOR,\n",
    "               pooling_factor=configs.InputsConfig.POOLING_FACTOR,\n",
    "               hop_length=configs.InputsConfig.HOP_LENGTH,\n",
    "               sr=configs.InputsConfig.SAMPLING_RATE,\n",
    "               window=configs.InputsConfig.WINDOW):\n",
    "    \n",
    "    \n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss() #nn.MSELoss() #BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    train_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    writer_train = SummaryWriter(\"../graphs/train\")\n",
    "    writer_test = SummaryWriter(\"../graphs/validation\")\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    \"==================================MAIN LOOP==================================\"    \n",
    "    for epoch in range(1, epochs):\n",
    "        model.train()\n",
    "    \n",
    "        training_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        training_accuracy = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        F_train = 0.0\n",
    "        R_train = 0.0\n",
    "        P_train = 0.0\n",
    "\n",
    "        val_loss = 0.0\n",
    "        validation_accuracy = 0.0\n",
    "\n",
    "        examples = 0\n",
    "        pbar = tqdm(total = len(trainloader))\n",
    "        print(\"Epoch:\", epoch)\n",
    "        \"==============================TRAIN LOOP=================================\"\n",
    "        optimizer.zero_grad()\n",
    "        for batch_idx, (images_mls, labels_mls, labels_sec_mls) in enumerate(trainloader):\n",
    "\n",
    "            images_mls = images_mls.to(device)\n",
    "\n",
    "            outputs_combined = model(images_mls.float())\n",
    "\n",
    "            outputs = outputs_combined.view(-1) #2º valor son las clases de salida\n",
    "            labels = labels_mls.view(-1).float().to(device) *0.98 + 0.01\n",
    "\n",
    "            train_loss = criterion(outputs, labels)\n",
    "            train_loss.backward()\n",
    "\n",
    "            if batch_idx % iterations == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            #Loss\n",
    "            training_loss += train_loss\n",
    "            running_loss = 0.99 * running_loss + (1 - 0.99) * train_loss\n",
    "\n",
    "            #Accuracy\n",
    "            accuracy = ((outputs > 0.5).float() == labels).float().mean()\n",
    "            training_accuracy += accuracy\n",
    "            running_accuracy = 0.99 * running_accuracy + (1 - 0.99) * accuracy\n",
    "            pbar.set_postfix(loss=running_loss.item()/(1-0.99**(batch_idx+1)), accuracy=running_accuracy.item()/(1-0.99**(batch_idx+1)))\n",
    "            pbar.update()\n",
    "            writer_train.add_scalar('Output Max', torch.sigmoid(outputs.max()), batch_idx)\n",
    "\n",
    "            \"\"\"This is only to graph results in tensorboard\"\"\"\n",
    "            labels_sec_mls = labels_sec_mls[0,:]\n",
    "            labels_sec_mls = labels_sec_mls[1:]\n",
    "            reference = np.array((np.copy(labels_sec_mls[:-1]), np.copy(labels_sec_mls[1:]))).T\n",
    "            repeated_list = []\n",
    "            for j in range(reference.shape[0]):\n",
    "                if reference[j,0] == reference[j,1]:\n",
    "                    repeated_list.append(j)\n",
    "            reference = np.delete(reference, repeated_list, 0)\n",
    "\n",
    "            pred_new = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "\n",
    "            delta = (pred_new.max() - pred_new.min())*0.2 + pred_new.min() #threshold\n",
    "\n",
    "            peak_position = signal.find_peaks(pred_new, height=delta, distance=lamda)[0] #array of peaks\n",
    "            peaks_positions = ((peak_position-padding_factor)*pooling_factor*hop_length)/sr\n",
    "            for i in range(len(peaks_positions)):\n",
    "                if peaks_positions[i] < 0:\n",
    "                    peaks_positions[i] = 0\n",
    "\n",
    "            pred_positions = np.array((np.copy(peaks_positions[:-1]), np.copy(peaks_positions[1:]))).T\n",
    "            repeated_list = []\n",
    "            for j in range(pred_positions.shape[0]):\n",
    "                if pred_positions[j,0] == pred_positions[j,1]:\n",
    "                    repeated_list.append(j)\n",
    "            pred_positions = np.delete(pred_positions, repeated_list, 0)\n",
    "\n",
    "            P_ant_train, R_ant_train, F_ant_train, *_ = mir_eval.segment.detection(reference, pred_positions, window=window, beta=1.0, trim=False)\n",
    "            P_train, R_train, F_train = P_train + P_ant_train, R_train + R_ant_train, F_train + F_ant_train\n",
    "           \n",
    "\n",
    "        pbar.close()\n",
    "        print(\"training_accuracy={:.2f}  training_loss={:.2f}\\n\".format(training_accuracy / len(trainloader), training_loss / len(trainloader)))\n",
    "\n",
    "        writer_train.add_scalar('Loss', training_loss / len(trainloader), epoch)\n",
    "        writer_train.add_scalar('Accuracy', training_accuracy / len(trainloader), epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        F = 0.0\n",
    "        R = 0.0\n",
    "        P = 0.0\n",
    "\n",
    "        \"============================VALIDATION LOOP==============================\"\n",
    "        with torch.no_grad(): #para que no calcule gradientes en validación\n",
    "            for batch_idx, (images_mls, labels_mls, labels_sec_mls) in enumerate(valloader):\n",
    "                #Forward pass\n",
    "                images_mls = images_mls.to(device)\n",
    "\n",
    "                val_outputs_combined = model(images_mls.float())\n",
    "                val_outputs = val_outputs_combined.view(-1) #2º valor son las clases de salida\n",
    "                labels = labels_mls.view(-1).float().to(device) *0.98 + 0.01\n",
    "\n",
    "                labels_sec_mls = labels_sec_mls[0,:]\n",
    "                reference = np.array((np.copy(labels_sec_mls[:-1]), np.copy(labels_sec_mls[1:]))).T\n",
    "                repeated_list = []\n",
    "                for j in range(reference.shape[0]):\n",
    "                    if reference[j,0] == reference[j,1]:\n",
    "                        repeated_list.append(j)\n",
    "                reference = np.delete(reference, repeated_list, 0)\n",
    "\n",
    "                pred_new = torch.sigmoid(val_outputs).cpu().detach().numpy()\n",
    "\n",
    "                delta = (pred_new.max() - pred_new.min())*0.2 + pred_new.min() #threshold\n",
    "\n",
    "                peak_position = signal.find_peaks(pred_new, height=delta, distance=lamda)[0] #array of peaks\n",
    "                peaks_positions = ((peak_position-padding_factor)*pooling_factor*hop_length)/sr\n",
    "                for i in range(len(peaks_positions)):\n",
    "                    if peaks_positions[i] < 0:\n",
    "                        peaks_positions[i] = 0\n",
    "\n",
    "                peaks_position = np.insert(peaks_positions, 0, 0)\n",
    "                peaks_position = np.append(peaks_position, len(pred_new))\n",
    "                pred_positions = np.array((np.copy(peaks_position[:-1]), np.copy(peaks_position[1:]))).T\n",
    "                repeated_list = []\n",
    "                for j in range(pred_positions.shape[0]):\n",
    "                    if pred_positions[j,0] == pred_positions[j,1]:\n",
    "                        repeated_list.append(j)\n",
    "                pred_positions = np.delete(pred_positions, repeated_list, 0)\n",
    "\n",
    "                P_ant, R_ant, F_ant, *_ = mir_eval.segment.detection(reference, pred_positions, window=window, beta=1.0, trim=False)\n",
    "                P, R, F = P + P_ant, R + R_ant, F + F_ant\n",
    "\n",
    "                val_loss_size = criterion(val_outputs, labels)\n",
    "\n",
    "                #Loss\n",
    "                val_loss += val_loss_size\n",
    "\n",
    "                #Accuracy\n",
    "                val_accuracy = ((val_outputs > 0.5).float() == labels).float().mean()\n",
    "                validation_accuracy += val_accuracy\n",
    "\n",
    "            print(\"validation_accuracy={:.2f}  validation_loss={:.2f}\\n\".format(validation_accuracy / len(valloader), val_loss / len(valloader)))\n",
    "\n",
    "            writer_test.add_scalar('Loss', val_loss / len(valloader), epoch) #epoch* len(valloader) + batch_idx\n",
    "            writer_test.add_scalar('Accuracy', validation_accuracy / len(valloader), epoch)\n",
    "\n",
    "            n_songs = len(valloader)\n",
    "            P_total, R_total, F_total = P/n_songs, R/n_songs, F/n_songs\n",
    "            writer_test.add_scalar('R', R_total, epoch)\n",
    "            writer_test.add_scalar('P', P_total, epoch)\n",
    "            writer_test.add_scalar('F', F_total, epoch)\n",
    "\n",
    "\n",
    "        #save trained model every 5 epochs\n",
    "        if not os.path.exists(configs.ParamsConfig.WEIGHTS_PATH):\n",
    "            os.mkdir(configs.ParamsConfig.WEIGHTS_PATH)\n",
    "\n",
    "        if epoch % save_epoch == 0:\n",
    "            torch.save(model.state_dict(), configs.ParamsConfig.WEIGHTS_PATH + \"saved_model_\" + str(epoch) + \"epochs.bin\")\n",
    "\n",
    "    writer_train.close()\n",
    "    writer_test.close()\n",
    "\n",
    "\n",
    "    print(\"Training finished in {:.2f}s\".format(time.time() - training_start_time))\n",
    "    #Se guardan los pesos de entrenamiento\n",
    "    torch.save(model.state_dict(), configs.ParamsConfig.WEIGHTS_PATH + \"saved_model_\" + str(epoch) + \"epochs.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gentle-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter, FileWriter\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import mir_eval\n",
    "import os\n",
    "\n",
    "def run_training(model, \n",
    "                 batch_size=configs.ParamsConfig.BATCH_SIZE, \n",
    "                  epochs=configs.ParamsConfig.NUM_EPOCHS, \n",
    "                  save_epoch=5,\n",
    "                  lr=configs.ParamsConfig.LEARNING_RATE, \n",
    "                  iterations=configs.ParamsConfig.ITERATIONS,\n",
    "                  lamda=configs.InputsConfig.LAMBDA,\n",
    "                  output_channels=configs.ParamsConfig.OUT_CHANNELS, \n",
    "                  padding_factor=configs.InputsConfig.PADDING_FACTOR,\n",
    "                  pooling_factor=configs.InputsConfig.POOLING_FACTOR,\n",
    "                  hop_length=configs.InputsConfig.HOP_LENGTH,\n",
    "                  sr=configs.InputsConfig.SAMPLING_RATE,\n",
    "                  window=configs.InputsConfig.WINDOW,\n",
    "                  labels_path=configs.PathsConfig.LABELS_PATH):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if model == \"mels\":\n",
    "        input_train_path = configs.PathsConfig.MELS_TRAIN_PATH\n",
    "        input_val_path = configs.PathsConfig.MELS_VAL_PATH\n",
    "        \n",
    "        mels_train_dataset, mels_trainloader = dataloaders.build_dataloader(batch_size, input_train_path, labels_path)\n",
    "        mels_val_dataset, mels_valloader = dataloaders.build_dataloader(batch_size, input_val_path, labels_path)\n",
    "        \n",
    "        assert len(mels_train_dataset) > 0\n",
    "        assert len(mels_val_dataset) > 0\n",
    "        \n",
    "        model = model_CNN_MLS.CNN_Fusion(output_channels, output_channels).to(device)\n",
    "\n",
    "    print(\"Input data imported in {:.2f}s\".format(time.time() - start_time))\n",
    "\n",
    "    print('============================MODEL=====================================')\n",
    "    print(\"Total SSLMs cargadas para entrenamiento:\", len(mels_trainloader)*batch_size)\n",
    "    print(\"Total SSLMs cargadas para validacion:\", len(mels_valloader)*batch_size)\n",
    "    \n",
    "    print(\"==========================TRAINING====================================\")\n",
    "            \n",
    "    train_loop(model=model, \n",
    "               trainloader=mels_trainloader, \n",
    "               valloader=mels_valloader, \n",
    "               device=device, \n",
    "               save_epoch=save_epoch, \n",
    "               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aboriginal-arkansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/650 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data imported in 3.46s\n",
      "============================MODEL=====================================\n",
      "Total SSLMs cargadas para entrenamiento: 650\n",
      "Total SSLMs cargadas para validacion: 150\n",
      "==========================TRAINING====================================\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 650/650 [00:56<00:00, 11.57it/s, accuracy=0, loss=0.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy=0.00  training_loss=0.30\n",
      "\n",
      "validation_accuracy=0.00  validation_loss=0.28\n",
      "\n",
      "Training finished in 63.91s\n"
     ]
    }
   ],
   "source": [
    "model = \"mels\"\n",
    "\n",
    "run_training(model=model, epochs=2, save_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-italy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-family",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
